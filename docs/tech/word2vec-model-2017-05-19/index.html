<!doctype html>
<html lang="en">
<title>  - Raymond ZHAO WENLONG</title>
<link rel="stylesheet" href="/style.css">

<nav>
    <h1><small><a href="/index.html">Raymond ZHAO WENLONG</a></small></h1>
    <ul>
        <li><a href="/index.html"> Blog </a>
        <li><a href="/slides.html"> Slides </a>
        <li><a href="/bookshelf.html"> Bookshelf </a>
        <li><a href="/about.html"> About </a>
    </ul>
</nav>

<section class="content">
    <!--
    <header>
        
<title> Word2Vec Model</title>

      </header>
      -->
    
<article class="post">
    <header>
        <h1><strong>Word2Vec Model</strong></h1>
    </header>
    <p class="body"><h4>distributed representations of words</h4>
<p>word2vec is a class of neural network models, trained on unlabelled trainign corpus, that produce a vector for each word in the corpus that encode its valuable syntactic and  semantic informaton .</p>
<h4>Skip-grams (SG) - predict context words given target (position independent)</h4>
<h4>Continuous Bag of Words (CBOW) - Predict target word from bag-of-words context</h4>
<h4>Stochastic Gradient Descent</h4>
<h4>reference</h4>
<ul>
<li>Linguistic Regularities in Continuous Space Word Representations,</li>
<li><a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/">The amazing power of word vectors</a></li>
<li><a href="http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_I_The_Skip-Gram_Model.pdf">Word2vec tutorial</a></li>
<li><a href="http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html">Deep Learning Basics: Neural Networks, Backpropagation and Stochastic Gradient Descent</a></li>
</ul>
</p>
</article>




</section>


<div class="footer">
    <ul>
        <small>&copy; 2021 zhaowenlong </small>
    </ul>
</div>